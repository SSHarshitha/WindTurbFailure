{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ed8b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d476c2a1-433b-4f72-8dbc-f656fecd2619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4180ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate the features and target variable\n",
    "X=df.drop(['Failure_status','date'],axis=1) #Exclude Failure status\n",
    "y=df['Failure_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ea22e56-42a0-4f97-a6da-f78f3285afb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    1.0\n",
       "2    1.0\n",
       "3    1.0\n",
       "4    1.0\n",
       "Name: Failure_status, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b38190dd-4b43-469f-886c-1049f7a27ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2895    1.0\n",
       "2896    0.0\n",
       "2897    1.0\n",
       "2898    0.0\n",
       "2899    1.0\n",
       "Name: Failure_status, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f9c05",
   "metadata": {},
   "source": [
    "# Building ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c67e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e37bf2eb-9733-40fc-bcfa-9db0fc4b376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable if it is categorical\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5713224a-74c7-4881-860e-a669393aacd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifiers to perform transformations\n",
    "classifiers = [\n",
    "    {'name': 'Random Forest', 'model': RandomForestClassifier(), 'params': {'n_estimators': [10, 50, 100]}},\n",
    "    {'name': 'SVM', 'model': SVC(), 'params': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}},\n",
    "    {'name': 'KNN', 'model': KNeighborsClassifier(), 'params': {'n_neighbors': [3, 5, 7]}},\n",
    "    {'name': 'Decision Tree', 'model': DecisionTreeClassifier(), 'params': {'max_depth': [None, 5, 10]}},\n",
    "    {'name': 'Naive Bayes', 'model': GaussianNB(), 'params': {}},\n",
    "    {'name': 'AdaBoost', 'model': AdaBoostClassifier(), 'params': {'n_estimators': [50, 100, 200]}},\n",
    "    {'name': 'Ridge Regression', 'model': RidgeClassifier(), 'params': {'alpha': [0.1, 1, 10]}},\n",
    "    {'name': 'Lasso Regression', 'model': Lasso(), 'params': {'alpha': [0.1, 1, 10]}},\n",
    "    {'name': 'Logistic Regression', 'model': LogisticRegression(), 'params': {'C': [0.1, 1, 10]}},\n",
    "    {'name': 'MLP', 'model': MLPClassifier(), 'params': {'hidden_layer_sizes': [(50,), (100,), (50, 50)]}}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c398874d-19d6-4f6b-916d-34e9a3209188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the results\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d529f9ea-7425-40f6-bab0-4b90da184b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy for Random Forest: 1.0\n",
      "Train Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       404\n",
      "           1       1.00      1.00      1.00      1916\n",
      "\n",
      "    accuracy                           1.00      2320\n",
      "   macro avg       1.00      1.00      1.00      2320\n",
      "weighted avg       1.00      1.00      1.00      2320\n",
      "\n",
      "Test Accuracy for Random Forest: 1.0\n",
      "Test Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        96\n",
      "           1       1.00      1.00      1.00       484\n",
      "\n",
      "    accuracy                           1.00       580\n",
      "   macro avg       1.00      1.00      1.00       580\n",
      "weighted avg       1.00      1.00      1.00       580\n",
      "\n",
      "\n",
      "Train Accuracy for SVM: 1.0\n",
      "Train Classification Report for SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       404\n",
      "           1       1.00      1.00      1.00      1916\n",
      "\n",
      "    accuracy                           1.00      2320\n",
      "   macro avg       1.00      1.00      1.00      2320\n",
      "weighted avg       1.00      1.00      1.00      2320\n",
      "\n",
      "Test Accuracy for SVM: 1.0\n",
      "Test Classification Report for SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        96\n",
      "           1       1.00      1.00      1.00       484\n",
      "\n",
      "    accuracy                           1.00       580\n",
      "   macro avg       1.00      1.00      1.00       580\n",
      "weighted avg       1.00      1.00      1.00       580\n",
      "\n",
      "\n",
      "Train Accuracy for KNN: 1.0\n",
      "Train Classification Report for KNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       404\n",
      "           1       1.00      1.00      1.00      1916\n",
      "\n",
      "    accuracy                           1.00      2320\n",
      "   macro avg       1.00      1.00      1.00      2320\n",
      "weighted avg       1.00      1.00      1.00      2320\n",
      "\n",
      "Test Accuracy for KNN: 1.0\n",
      "Test Classification Report for KNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        96\n",
      "           1       1.00      1.00      1.00       484\n",
      "\n",
      "    accuracy                           1.00       580\n",
      "   macro avg       1.00      1.00      1.00       580\n",
      "weighted avg       1.00      1.00      1.00       580\n",
      "\n",
      "\n",
      "Train Accuracy for Decision Tree: 1.0\n",
      "Train Classification Report for Decision Tree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       404\n",
      "           1       1.00      1.00      1.00      1916\n",
      "\n",
      "    accuracy                           1.00      2320\n",
      "   macro avg       1.00      1.00      1.00      2320\n",
      "weighted avg       1.00      1.00      1.00      2320\n",
      "\n",
      "Test Accuracy for Decision Tree: 1.0\n",
      "Test Classification Report for Decision Tree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        96\n",
      "           1       1.00      1.00      1.00       484\n",
      "\n",
      "    accuracy                           1.00       580\n",
      "   macro avg       1.00      1.00      1.00       580\n",
      "weighted avg       1.00      1.00      1.00       580\n",
      "\n",
      "\n",
      "Train Accuracy for Naive Bayes: 0.9952586206896552\n",
      "Train Classification Report for Naive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       404\n",
      "           1       1.00      0.99      1.00      1916\n",
      "\n",
      "    accuracy                           1.00      2320\n",
      "   macro avg       0.99      1.00      0.99      2320\n",
      "weighted avg       1.00      1.00      1.00      2320\n",
      "\n",
      "Test Accuracy for Naive Bayes: 0.996551724137931\n",
      "Test Classification Report for Naive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        96\n",
      "           1       1.00      1.00      1.00       484\n",
      "\n",
      "    accuracy                           1.00       580\n",
      "   macro avg       0.99      1.00      0.99       580\n",
      "weighted avg       1.00      1.00      1.00       580\n",
      "\n",
      "\n",
      "Train Accuracy for AdaBoost: 1.0\n",
      "Train Classification Report for AdaBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       404\n",
      "           1       1.00      1.00      1.00      1916\n",
      "\n",
      "    accuracy                           1.00      2320\n",
      "   macro avg       1.00      1.00      1.00      2320\n",
      "weighted avg       1.00      1.00      1.00      2320\n",
      "\n",
      "Test Accuracy for AdaBoost: 1.0\n",
      "Test Classification Report for AdaBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        96\n",
      "           1       1.00      1.00      1.00       484\n",
      "\n",
      "    accuracy                           1.00       580\n",
      "   macro avg       1.00      1.00      1.00       580\n",
      "weighted avg       1.00      1.00      1.00       580\n",
      "\n",
      "\n",
      "Train Accuracy for Ridge Regression: 1.0\n",
      "Train Classification Report for Ridge Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       404\n",
      "           1       1.00      1.00      1.00      1916\n",
      "\n",
      "    accuracy                           1.00      2320\n",
      "   macro avg       1.00      1.00      1.00      2320\n",
      "weighted avg       1.00      1.00      1.00      2320\n",
      "\n",
      "Test Accuracy for Ridge Regression: 1.0\n",
      "Test Classification Report for Ridge Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        96\n",
      "           1       1.00      1.00      1.00       484\n",
      "\n",
      "    accuracy                           1.00       580\n",
      "   macro avg       1.00      1.00      1.00       580\n",
      "weighted avg       1.00      1.00      1.00       580\n",
      "\n",
      "\n",
      "Train Accuracy for Lasso Regression: 1.0\n",
      "Train Classification Report for Lasso Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       404\n",
      "           1       1.00      1.00      1.00      1916\n",
      "\n",
      "    accuracy                           1.00      2320\n",
      "   macro avg       1.00      1.00      1.00      2320\n",
      "weighted avg       1.00      1.00      1.00      2320\n",
      "\n",
      "Test Accuracy for Lasso Regression: 1.0\n",
      "Test Classification Report for Lasso Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        96\n",
      "           1       1.00      1.00      1.00       484\n",
      "\n",
      "    accuracy                           1.00       580\n",
      "   macro avg       1.00      1.00      1.00       580\n",
      "weighted avg       1.00      1.00      1.00       580\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy for Logistic Regression: 1.0\n",
      "Train Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       404\n",
      "           1       1.00      1.00      1.00      1916\n",
      "\n",
      "    accuracy                           1.00      2320\n",
      "   macro avg       1.00      1.00      1.00      2320\n",
      "weighted avg       1.00      1.00      1.00      2320\n",
      "\n",
      "Test Accuracy for Logistic Regression: 0.9982758620689656\n",
      "Test Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        96\n",
      "           1       1.00      1.00      1.00       484\n",
      "\n",
      "    accuracy                           1.00       580\n",
      "   macro avg       1.00      0.99      1.00       580\n",
      "weighted avg       1.00      1.00      1.00       580\n",
      "\n",
      "\n",
      "Train Accuracy for MLP: 1.0\n",
      "Train Classification Report for MLP:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       404\n",
      "           1       1.00      1.00      1.00      1916\n",
      "\n",
      "    accuracy                           1.00      2320\n",
      "   macro avg       1.00      1.00      1.00      2320\n",
      "weighted avg       1.00      1.00      1.00      2320\n",
      "\n",
      "Test Accuracy for MLP: 1.0\n",
      "Test Classification Report for MLP:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        96\n",
      "           1       1.00      1.00      1.00       484\n",
      "\n",
      "    accuracy                           1.00       580\n",
      "   macro avg       1.00      1.00      1.00       580\n",
      "weighted avg       1.00      1.00      1.00       580\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the models\n",
    "def evaluate_classification(y_true, y_pred):\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred_binary)\n",
    "    report = classification_report(y_true, y_pred_binary)\n",
    "    return acc, report\n",
    "\n",
    "\n",
    "# Train the model and make predictions   \n",
    "for classifier in classifiers:\n",
    "    model = GridSearchCV(classifier['model'], classifier['params'], cv=5)\n",
    "    model.fit(X_train, y_train)\n",
    "    best_params = model.best_params_\n",
    "    best_model = classifier['model'].set_params(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    #Calculate accuracy\n",
    "    train_accuracy, train_classification_report = evaluate_classification(y_train, y_train_pred)\n",
    "    test_accuracy, test_classification_report = evaluate_classification(y_test, y_test_pred)\n",
    "\n",
    "    \n",
    "    results[classifier['name']] = {\n",
    "        'best_params': best_params,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'train_classification_report': train_classification_report,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_classification_report': test_classification_report\n",
    "    }\n",
    "\n",
    "    print(f\"Train Accuracy for {classifier['name']}: {train_accuracy}\")\n",
    "    print(f\"Train Classification Report for {classifier['name']}:\\n{train_classification_report}\")\n",
    "    print(f\"Test Accuracy for {classifier['name']}: {test_accuracy}\")\n",
    "    print(f\"Test Classification Report for {classifier['name']}:\\n{test_classification_report}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f94a5ba-db5b-47a1-a1f5-c4f98d71315e",
   "metadata": {},
   "source": [
    "Saving the best model - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4057080b-cb84-4b9b-adee-a0e5b297f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf57e3d2-5eb3-4e39-aa60-2e033c1eecb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9977011494252873 \n",
      "\n",
      "Train accuracy:  0.9947261663286004\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GridSearchCV(GaussianNB(), {}, cv=5)\n",
    "model.fit(X_train, y_train)\n",
    "best_params = model.best_params_\n",
    "best_model = GaussianNB().set_params(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_acc=accuracy_score(y_test,y_test_pred)\n",
    "train_acc=accuracy_score(y_train,y_train_pred)\n",
    "print(\"Test accuracy: \", test_acc,\"\\n\")\n",
    "print(\"Train accuracy: \",train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab05322e-13c3-46e4-84e0-3af9491bb8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "with open('trained_model.sav', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
